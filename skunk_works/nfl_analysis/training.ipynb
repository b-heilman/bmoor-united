{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Couldn't find program: 'sh'\n"
     ]
    }
   ],
   "source": [
    "%%sh\n",
    "\n",
    "npm run test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: numpy in /home/bheilman/.local/lib/python3.8/site-packages (1.23.5)\n",
      "Requirement already satisfied: pandas in /home/bheilman/.local/lib/python3.8/site-packages (1.5.2)\n",
      "Requirement already satisfied: scikit-learn in /home/bheilman/.local/lib/python3.8/site-packages (1.2.0)\n",
      "Requirement already satisfied: sklearn in /home/bheilman/.local/lib/python3.8/site-packages (0.0.post1)\n",
      "Requirement already satisfied: xgboost in /home/bheilman/.local/lib/python3.8/site-packages (1.7.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in /home/bheilman/.local/lib/python3.8/site-packages (from pandas) (2022.6)\n",
      "Requirement already satisfied: python-dateutil>=2.8.1 in /home/bheilman/.local/lib/python3.8/site-packages (from pandas) (2.8.2)\n",
      "Requirement already satisfied: scipy>=1.3.2 in /home/bheilman/.local/lib/python3.8/site-packages (from scikit-learn) (1.9.3)\n",
      "Requirement already satisfied: joblib>=1.1.1 in /home/bheilman/.local/lib/python3.8/site-packages (from scikit-learn) (1.2.0)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in /home/bheilman/.local/lib/python3.8/site-packages (from scikit-learn) (3.1.0)\n",
      "Requirement already satisfied: six>=1.5 in /usr/lib/python3/dist-packages (from python-dateutil>=2.8.1->pandas) (1.14.0)\n"
     ]
    }
   ],
   "source": [
    "# https://jakevdp.github.io/blog/2017/12/05/installing-python-packages-from-jupyter/\n",
    "import sys\n",
    "\n",
    "!{sys.executable} -m pip install numpy pandas scikit-learn sklearn xgboost "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train_x Shape: (327, 8)\n",
      "Train_y Shape: (327,)\n",
      "Val_x Shape: (82, 8)\n",
      "Val_y Shape: (82,)\n",
      "Test_x Shape: (103, 8)\n",
      "Test_y Shape: (103,)\n"
     ]
    },
    {
     "ename": "ImportError",
     "evalue": "sklearn needs to be installed in order to use this module",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[11], line 185\u001b[0m\n\u001b[1;32m    182\u001b[0m         \u001b[39mprint\u001b[39m(json\u001b[39m.\u001b[39mdump(stats, ensure_ascii\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m, indent\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m\\t\u001b[39;00m\u001b[39m\"\u001b[39m, skipkeys\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m))\n\u001b[1;32m    184\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39m__name__\u001b[39m \u001b[39m==\u001b[39m \u001b[39m\"\u001b[39m\u001b[39m__main__\u001b[39m\u001b[39m\"\u001b[39m:\n\u001b[0;32m--> 185\u001b[0m     run()\n",
      "Cell \u001b[0;32mIn[11], line 178\u001b[0m, in \u001b[0;36mrun\u001b[0;34m()\u001b[0m\n\u001b[1;32m    175\u001b[0m \u001b[39mfor\u001b[39;00m season \u001b[39min\u001b[39;00m seasons:\n\u001b[1;32m    176\u001b[0m     info \u001b[39m=\u001b[39m create_training_info(load_training_data(season))\n\u001b[0;32m--> 178\u001b[0m     model \u001b[39m=\u001b[39m create_model(info)\n\u001b[1;32m    180\u001b[0m     stats \u001b[39m=\u001b[39m calc_statistics(info, model)\n\u001b[1;32m    182\u001b[0m     \u001b[39mprint\u001b[39m(json\u001b[39m.\u001b[39mdump(stats, ensure_ascii\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m, indent\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m\\t\u001b[39;00m\u001b[39m\"\u001b[39m, skipkeys\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m))\n",
      "Cell \u001b[0;32mIn[11], line 60\u001b[0m, in \u001b[0;36mcreate_model\u001b[0;34m(info)\u001b[0m\n\u001b[1;32m     58\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mcreate_model\u001b[39m(info):\n\u001b[1;32m     59\u001b[0m     \u001b[39m# ---- Model\u001b[39;00m\n\u001b[0;32m---> 60\u001b[0m     model \u001b[39m=\u001b[39m xbc(objective\u001b[39m=\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39mbinary:logistic\u001b[39;49m\u001b[39m\"\u001b[39;49m)\n\u001b[1;32m     62\u001b[0m     model\u001b[39m.\u001b[39mfit(\n\u001b[1;32m     63\u001b[0m         info[\u001b[39m\"\u001b[39m\u001b[39mx\u001b[39m\u001b[39m\"\u001b[39m][\u001b[39m\"\u001b[39m\u001b[39mtraining\u001b[39m\u001b[39m\"\u001b[39m]\u001b[39m.\u001b[39mvalues,\n\u001b[1;32m     64\u001b[0m         le\u001b[39m.\u001b[39mfit_transform(info[\u001b[39m\"\u001b[39m\u001b[39my\u001b[39m\u001b[39m\"\u001b[39m][\u001b[39m\"\u001b[39m\u001b[39mtraining\u001b[39m\u001b[39m\"\u001b[39m]),\n\u001b[1;32m     65\u001b[0m         eval_set\u001b[39m=\u001b[39m[(info[\u001b[39m\"\u001b[39m\u001b[39mx\u001b[39m\u001b[39m\"\u001b[39m][\u001b[39m\"\u001b[39m\u001b[39mvalidation\u001b[39m\u001b[39m\"\u001b[39m]\u001b[39m.\u001b[39mvalues, info[\u001b[39m\"\u001b[39m\u001b[39my\u001b[39m\u001b[39m\"\u001b[39m][\u001b[39m\"\u001b[39m\u001b[39mvalidation\u001b[39m\u001b[39m\"\u001b[39m])],\n\u001b[1;32m     66\u001b[0m     )\n\u001b[1;32m     68\u001b[0m     \u001b[39mprint\u001b[39m(\u001b[39m\"\u001b[39m\u001b[39m-- model created --\u001b[39m\u001b[39m\"\u001b[39m)\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/xgboost/core.py:620\u001b[0m, in \u001b[0;36mrequire_keyword_args.<locals>.throw_if.<locals>.inner_f\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    618\u001b[0m \u001b[39mfor\u001b[39;00m k, arg \u001b[39min\u001b[39;00m \u001b[39mzip\u001b[39m(sig\u001b[39m.\u001b[39mparameters, args):\n\u001b[1;32m    619\u001b[0m     kwargs[k] \u001b[39m=\u001b[39m arg\n\u001b[0;32m--> 620\u001b[0m \u001b[39mreturn\u001b[39;00m func(\u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/xgboost/sklearn.py:1422\u001b[0m, in \u001b[0;36mXGBClassifier.__init__\u001b[0;34m(self, objective, use_label_encoder, **kwargs)\u001b[0m\n\u001b[1;32m   1420\u001b[0m \u001b[39mif\u001b[39;00m use_label_encoder \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m   1421\u001b[0m     warnings\u001b[39m.\u001b[39mwarn(\u001b[39m\"\u001b[39m\u001b[39m`use_label_encoder` is deprecated in 1.7.0.\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m-> 1422\u001b[0m \u001b[39msuper\u001b[39;49m()\u001b[39m.\u001b[39;49m\u001b[39m__init__\u001b[39;49m(objective\u001b[39m=\u001b[39;49mobjective, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/xgboost/sklearn.py:584\u001b[0m, in \u001b[0;36mXGBModel.__init__\u001b[0;34m(self, max_depth, max_leaves, max_bin, grow_policy, learning_rate, n_estimators, verbosity, objective, booster, tree_method, n_jobs, gamma, min_child_weight, max_delta_step, subsample, sampling_method, colsample_bytree, colsample_bylevel, colsample_bynode, reg_alpha, reg_lambda, scale_pos_weight, base_score, random_state, missing, num_parallel_tree, monotone_constraints, interaction_constraints, importance_type, gpu_id, validate_parameters, predictor, enable_categorical, feature_types, max_cat_to_onehot, max_cat_threshold, eval_metric, early_stopping_rounds, callbacks, **kwargs)\u001b[0m\n\u001b[1;32m    540\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__init__\u001b[39m(\n\u001b[1;32m    541\u001b[0m     \u001b[39mself\u001b[39m,\n\u001b[1;32m    542\u001b[0m     max_depth: Optional[\u001b[39mint\u001b[39m] \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    581\u001b[0m     \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs: Any,\n\u001b[1;32m    582\u001b[0m ) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    583\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m SKLEARN_INSTALLED:\n\u001b[0;32m--> 584\u001b[0m         \u001b[39mraise\u001b[39;00m \u001b[39mImportError\u001b[39;00m(\n\u001b[1;32m    585\u001b[0m             \u001b[39m\"\u001b[39m\u001b[39msklearn needs to be installed in order to use this module\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    586\u001b[0m         )\n\u001b[1;32m    587\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mn_estimators \u001b[39m=\u001b[39m n_estimators\n\u001b[1;32m    588\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mobjective \u001b[39m=\u001b[39m objective\n",
      "\u001b[0;31mImportError\u001b[0m: sklearn needs to be installed in order to use this module"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from xgboost import XGBClassifier as xbc\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.model_selection import train_test_split, cross_validate\n",
    "from sklearn.metrics import f1_score, accuracy_score, confusion_matrix\n",
    "\n",
    "le = LabelEncoder()\n",
    "\n",
    "# Defining S3 bucket names\n",
    "seasons = ['2019', '2020', '2021', '2022']\n",
    "\n",
    "def load_training_data(season):\n",
    "    incoming = json.load(open('./frames/{}.json'.format(season)))\n",
    "\n",
    "    return pd.DataFrame(incoming)\n",
    "\n",
    "\n",
    "def create_training_info(training_df):\n",
    "    # Split the\n",
    "    training_sets = training_df.drop(columns=[\"label\"])\n",
    "    training_values = training_df[\"label\"]\n",
    "\n",
    "    # _x => training sets\n",
    "    # _y => training values\n",
    "    train_x, test_x, train_y, test_y = train_test_split(\n",
    "        training_sets,\n",
    "        training_values,\n",
    "        test_size=0.2,\n",
    "        shuffle=True\n",
    "    )\n",
    "    train_x, val_x, train_y, val_y = train_test_split(\n",
    "        train_x,\n",
    "        train_y,\n",
    "        test_size=0.2,\n",
    "        shuffle=True\n",
    "    )\n",
    "\n",
    "    print(f\"Train_x Shape: {train_x.shape}\")\n",
    "\n",
    "    print(f\"Train_y Shape: {train_y.shape}\")\n",
    "\n",
    "    print(f\"Val_x Shape: {val_x.shape}\")\n",
    "\n",
    "    print(f\"Val_y Shape: {val_y.shape}\")\n",
    "\n",
    "    print(f\"Test_x Shape: {test_x.shape}\")\n",
    "\n",
    "    print(f\"Test_y Shape: {test_y.shape}\")\n",
    "\n",
    "    return {\n",
    "        \"x\": {\"training\": train_x, \"validation\": val_x, \"testing\": test_x},\n",
    "        \"y\": {\"training\": train_y, \"validation\": val_y, \"testing\": test_y},\n",
    "    }\n",
    "\n",
    "def create_model(info):\n",
    "    # ---- Model\n",
    "    model = xbc(objective=\"binary:logistic\")\n",
    "\n",
    "    model.fit(\n",
    "        info[\"x\"][\"training\"].values,\n",
    "        le.fit_transform(info[\"y\"][\"training\"]),\n",
    "        eval_set=[(info[\"x\"][\"validation\"].values, info[\"y\"][\"validation\"])],\n",
    "    )\n",
    "\n",
    "    print(\"-- model created --\")\n",
    "    return model\n",
    "\n",
    "def obtain_eer(y_true, pred_probs):\n",
    "    \"\"\"\n",
    "    Obtains the Equal Error Rate (EER) of a classification model based on its False Acceptance\n",
    "    Rate (FAR) and False Rejection Rate (FRR) curves generated from a set of predicted probabilities\n",
    "    of the model.\n",
    "\n",
    "    The FAR and FRR curves are obtained by varying the decision threshold of the model for the\n",
    "    positive class from 0 to 1 and recording each resulting perfomance in terms of FAR and FRR. The\n",
    "    EER will be the point where both curves intersect.\n",
    "\n",
    "    Parameters\n",
    "    __________\n",
    "        y_true (np.ndarray): Array with the true value of the class for each observation.\n",
    "        pred_probs (np.ndarray): Bidimensional array with the predicted probabilites for each class.\n",
    "\n",
    "    Returns\n",
    "    _______\n",
    "        df_res (pd.DataFrame): DataFrame containing performance results for each decision threshold.\n",
    "        eer (float): Resulting EER value of the model for the given predictions.\n",
    "\n",
    "    \"\"\"\n",
    "    # Extracting positive probabilities\n",
    "    pos_probs = pred_probs[:, 1]\n",
    "\n",
    "    # Obtaining number of total attempts\n",
    "    n_attempts = len(pos_probs)\n",
    "\n",
    "    # Defining output objects\n",
    "    fars, frrs, cut_prob = [], [], []\n",
    "\n",
    "    # Looping over all possible thresholds\n",
    "    for threshold in np.linspace(0, 1, 100):\n",
    "        # Making decision according to threshold\n",
    "        positives = (pos_probs > threshold).astype(int)\n",
    "\n",
    "        # Creating a single array with true and predicted values\n",
    "        df_cases = pd.DataFrame({\"true\": y_true.values, \"pred\": positives})\n",
    "\n",
    "        # Obtaning FAR and FRR\n",
    "        far = sum((df_cases[\"true\"] == 0) & (df_cases[\"pred\"] == 1)) / n_attempts\n",
    "        frr = sum((df_cases[\"true\"] == 1) & (df_cases[\"pred\"] == 0)) / n_attempts\n",
    "\n",
    "        # Appending results to output objects\n",
    "        fars.append(far)\n",
    "        frrs.append(frr)\n",
    "        cut_prob.append(threshold)\n",
    "\n",
    "    # Creating DF of results\n",
    "    df_res = pd.DataFrame({\"cut_prob\": cut_prob, \"far\": fars, \"frr\": frrs})\n",
    "    df_res[\"diff\"] = abs(df_res[\"far\"] - df_res[\"frr\"])\n",
    "\n",
    "    # Finding crossing point of FAR and FRR (EER)\n",
    "    df_eer = (\n",
    "        df_res[df_res[\"diff\"] == df_res[\"diff\"].min()]\n",
    "        .drop_duplicates(subset=\"diff\", keep=\"first\")\n",
    "        .reset_index(drop=True)\n",
    "    )\n",
    "    eer = (df_eer[\"far\"][0] + df_eer[\"frr\"][0]) / 2\n",
    "\n",
    "    return df_res, eer\n",
    "\n",
    "def calc_statistics(info, model):\n",
    "    predictions = model.predict(info[\"x\"][\"testing\"].values)\n",
    "    probability = model.predict_proba(info[\"x\"][\"testing\"].values)\n",
    "\n",
    "    acc_test = accuracy_score(info[\"y\"][\"testing\"], predictions)\n",
    "\n",
    "    f1_test = f1_score(info[\"y\"][\"testing\"], predictions)\n",
    "\n",
    "    # Inbuilt model feature importance\n",
    "\n",
    "    df_featimp = pd.DataFrame(\n",
    "        {\n",
    "            \"feature\": info[\"x\"][\"training\"].columns,\n",
    "            \"importance\": model.feature_importances_,\n",
    "        }\n",
    "    )\n",
    "\n",
    "    # Sorting by importance\n",
    "    df_featimp = df_featimp.sort_values(by=\"importance\", ascending=False)\n",
    "\n",
    "    top_20 = df_featimp.head(20)\n",
    "    bottom_20 = df_featimp.tail(20)\n",
    "\n",
    "    # Calculate EER\n",
    "    df_res, eer = obtain_eer(info[\"y\"][\"testing\"], probability)\n",
    "\n",
    "    return {\n",
    "        \"accuracy\": acc_test,\n",
    "        \"f1\": f1_test,\n",
    "        \"eer\": eer,\n",
    "        \"dimensions\": {\n",
    "            \"columns\": len(info[\"x\"][\"training\"].columns),\n",
    "            \"training\": len(info[\"x\"][\"training\"].values),\n",
    "            \"validation\": len(info[\"x\"][\"validation\"].values),\n",
    "            \"testing\": len(info[\"x\"][\"testing\"].values),\n",
    "        },\n",
    "        \"importance\": {\n",
    "            \"top\": top_20.to_dict(orient=\"records\"),\n",
    "            \"bottom\": bottom_20.to_dict(orient=\"records\"),\n",
    "        },\n",
    "    }\n",
    "\n",
    "def run():\n",
    "    for season in seasons:\n",
    "        info = create_training_info(load_training_data(season))\n",
    "\n",
    "        model = create_model(info)\n",
    "\n",
    "        stats = calc_statistics(info, model)\n",
    "\n",
    "        print(json.dumps(stats, ensure_ascii=False, indent=\"\\t\", skipkeys=True))\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "e7370f93d1d0cde622a1f8e1c04877d8463912d04d973331ad4851f04de6915a"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
